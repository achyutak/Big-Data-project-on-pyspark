{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "from pyspark.sql.functions import lit, col\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_arr = {}\n",
    "min_arr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/karth/Documents/Cloud Computing/Assignment 3/data/weather/\"\n",
    "years_arr = os.listdir(path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiated the results.txt file to capture results\n"
     ]
    }
   ],
   "source": [
    "stdoutOrigin=sys.stdout \n",
    "f = open(\"results.txt\", \"w\")\n",
    "sys.stdout = f\n",
    "print(\"\\n\\tSpark Project #2 \\n \\t submitted by: Aravinda Karthik Achyutuni\")\n",
    "sys.stdout = stdoutOrigin\n",
    "f.close()\n",
    "print(\"Initiated the results.txt file to capture results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..loading files from 2010 ...\n",
      "Loading Done! :D\n",
      "Dropping unnecessary columns..\n",
      "Done!\n",
      "Finding the hottest day of  2010 ..\n",
      "Done!\n",
      "Finding the coldest day of  2010 ..\n",
      "Done!\n",
      "Fetching and writing the  2010 data into txt file..\n",
      "Writing done successfully!\n",
      "..loading files from 2011 ...\n",
      "Loading Done! :D\n",
      "Dropping unnecessary columns..\n",
      "Done!\n",
      "Finding the hottest day of  2011 ..\n",
      "Done!\n",
      "Finding the coldest day of  2011 ..\n",
      "Done!\n",
      "Fetching and writing the  2011 data into txt file..\n",
      "Writing done successfully!\n",
      "..loading files from 2012 ...\n",
      "Loading Done! :D\n",
      "Dropping unnecessary columns..\n",
      "Done!\n",
      "Finding the hottest day of  2012 ..\n",
      "Done!\n",
      "Finding the coldest day of  2012 ..\n",
      "Done!\n",
      "Fetching and writing the  2012 data into txt file..\n",
      "Writing done successfully!\n",
      "..loading files from 2013 ...\n",
      "Loading Done! :D\n",
      "Dropping unnecessary columns..\n",
      "Done!\n",
      "Finding the hottest day of  2013 ..\n",
      "Done!\n",
      "Finding the coldest day of  2013 ..\n",
      "Done!\n",
      "Fetching and writing the  2013 data into txt file..\n",
      "Writing done successfully!\n",
      "..loading files from 2014 ...\n",
      "Loading Done! :D\n",
      "Dropping unnecessary columns..\n",
      "Done!\n",
      "Finding the hottest day of  2014 ..\n",
      "Done!\n",
      "Finding the coldest day of  2014 ..\n",
      "Done!\n",
      "Fetching and writing the  2014 data into txt file..\n",
      "Writing done successfully!\n",
      "..loading files from 2015 ...\n",
      "Loading Done! :D\n",
      "Dropping unnecessary columns..\n",
      "Done!\n",
      "Finding the hottest day of  2015 ..\n",
      "Done!\n",
      "Finding the coldest day of  2015 ..\n",
      "Done!\n",
      "Fetching and writing the  2015 data into txt file..\n",
      "Writing done successfully!\n",
      "..loading files from 2016 ...\n",
      "Loading Done! :D\n",
      "Dropping unnecessary columns..\n",
      "Done!\n",
      "Finding the hottest day of  2016 ..\n",
      "Done!\n",
      "Finding the coldest day of  2016 ..\n",
      "Done!\n",
      "Fetching and writing the  2016 data into txt file..\n",
      "Writing done successfully!\n",
      "..loading files from 2017 ...\n",
      "Loading Done! :D\n",
      "Dropping unnecessary columns..\n",
      "Done!\n",
      "Finding the hottest day of  2017 ..\n",
      "Done!\n",
      "Finding the coldest day of  2017 ..\n",
      "Done!\n",
      "Fetching and writing the  2017 data into txt file..\n",
      "Writing done successfully!\n",
      "..loading files from 2018 ...\n",
      "Loading Done! :D\n",
      "Dropping unnecessary columns..\n",
      "Done!\n",
      "Finding the hottest day of  2018 ..\n",
      "Done!\n",
      "Finding the coldest day of  2018 ..\n",
      "Done!\n",
      "Fetching and writing the  2018 data into txt file..\n",
      "Writing done successfully!\n",
      "..loading files from 2019 ...\n",
      "Loading Done! :D\n",
      "Dropping unnecessary columns..\n",
      "Done!\n",
      "Finding the hottest day of  2019 ..\n",
      "Done!\n",
      "Finding the coldest day of  2019 ..\n",
      "Done!\n",
      "Fetching and writing the  2019 data into txt file..\n",
      "Writing done successfully!\n"
     ]
    }
   ],
   "source": [
    "for year in years_arr:\n",
    "    print(\"..loading files from\",year,\"...\")\n",
    "    file_path = \"file///\"+path+year+\"/*.csv\"\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(file_path)\n",
    "    print(\"Loading Done! :D\")\n",
    "    print(\"Dropping unnecessary columns..\")\n",
    "    cleandata =  df.drop('LATITUDE','LONGITUDE','ELEVATION','NAME','TEMP','TEMP_ATTRIBUTES','DEWP','DEWP_ATTRIBUTES','SLP','SLP_ATTRIBUTES','VISIB','VISIB_ATTRIBUTES','WDSP','WDSP_ATTRIBUTES','MXSPD','SNDP','FSHTT')\n",
    "    print(\"Done!\")\n",
    "    print(\"Finding the hottest day of \",year,\"..\")\n",
    "    tmp1 = cleandata.filter(cleandata.MAX!=9999.9)\n",
    "    max_temp = tmp1.agg({\"MAX\": \"max\"}).collect()[0][0]\n",
    "    print(\"Done!\")\n",
    "    print(\"Finding the coldest day of \",year,\"..\")\n",
    "    tmp2 = cleandata.filter(cleandata.MIN!=9999.9)\n",
    "    min_temp = tmp2.agg({\"MIN\": \"min\"}).collect()[0][0]\n",
    "    print(\"Done!\")\n",
    "    print(\"Fetching and writing the \",year,\"data into txt file..\")\n",
    "    stdoutOrigin=sys.stdout \n",
    "    f = open(\"results.txt\", \"a\")\n",
    "    sys.stdout = f\n",
    "    print(\"Max temperature recorded in\",year,\"is\")\n",
    "    tmp1.select('STATION','DATE','MAX').filter(tmp1['MAX'] == lit(max_temp)).show(1)\n",
    "    print(\"Min temperature recorded in\",year,\"is\")\n",
    "    tmp2.select('STATION','DATE','MIN').filter(tmp2['MIN'] == lit(min_temp)).show(1)\n",
    "    sys.stdout = stdoutOrigin\n",
    "    f.close()\n",
    "    print(\"Writing done successfully!\")\n",
    "    max_arr[year] = max_temp\n",
    "    min_arr[year] = min_temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"..Prepping data for Hottest and Coldest day of the decade..\")\n",
    "max_year = max(max_arr.items(), key=operator.itemgetter(1))[0]\n",
    "min_year = min(min_arr.items(), key=operator.itemgetter(1))[0]\n",
    "file_path_max = \"file///\"+path+max_year+\"/*.csv\"\n",
    "file_path_min = \"file///\"+path+min_year+\"/*.csv\"\n",
    "minrdd = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(file_path_min)\n",
    "maxrdd = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(file_path_max)\n",
    "print(\"Prepping done!\")\n",
    "print(\"Writing the data into txt file..\")\n",
    "stdoutOrigin=sys.stdout \n",
    "f = open(\"results.txt\", \"a\")\n",
    "sys.stdout = f\n",
    "print(\"Hottest day of the decade is..\")\n",
    "maxrdd.select('STATION','DATE','MAX').filter(maxrdd['MAX'] == max_arr[max_year]).show(1)\n",
    "print(\"Coldest day of the Decade is..\")\n",
    "minrdd.select('STATION','DATE','MIN').filter(minrdd['MIN'] == min_arr[min_year]).show(1)\n",
    "sys.stdout = stdoutOrigin\n",
    "f.close()\n",
    "print(\"Writing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\".. Loading files from 2015 for PRCP data..\")\n",
    "file_path = \"file///\"+path+\"2015/*.csv\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(file_path)\n",
    "cleandata =  df.drop('LATITUDE','LONGITUDE','ELEVATION','NAME','TEMP','TEMP_ATTRIBUTES','DEWP','DEWP_ATTRIBUTES','SLP','SLP_ATTRIBUTES','VISIB','VISIB_ATTRIBUTES','WDSP','WDSP_ATTRIBUTES','MXSPD','SNDP','FSHTT')\n",
    "tmp = cleandata.filter(cleandata.PRCP!=99.99)\n",
    "print(\"loading and cleaning done!\")\n",
    "print(\"Fetching Max and Min Precipitation data..\")\n",
    "max_prc = tmp.agg({\"PRCP\": \"max\"}).collect()[0][0]\n",
    "min_prc = tmp.agg({\"PRCP\": \"min\"}).collect()[0][0]\n",
    "print(\"Fetching completed!\")\n",
    "print(\"Writing the data into txt file..\")\n",
    "stdoutOrigin=sys.stdout \n",
    "f = open(\"results.txt\", \"a\")\n",
    "sys.stdout = f\n",
    "print(\"Highest Precipitation in 2015 is at\")\n",
    "df.select('STATION','DATE','PRCP').filter(df['PRCP'] == lit(max_prc)).show(1)\n",
    "print(\"Lowest Precipitation in 2015 is at\")\n",
    "df.select('STATION','DATE','PRCP').filter(df['PRCP'] == lit(min_prc)).show(1)\n",
    "sys.stdout = stdoutOrigin\n",
    "f.close()\n",
    "print(\"Writing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\".. Loading files from 2019 for STP and Wind Gust Data\")\n",
    "file_path = \"file///\"+path+\"2019/*.csv\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(file_path)\n",
    "numerator = df.filter(df.PRCP==9999.9).count()\n",
    "tmp = df.filter(df.GUST!=999.9)\n",
    "max_WG = tmp.agg({\"GUST\": \"max\"}).collect()[0][0]\n",
    "denominator = df.count()\n",
    "print(\"Writing the data into txt file..\")\n",
    "stdoutOrigin=sys.stdout \n",
    "f = open(\"results.txt\", \"a\")\n",
    "sys.stdout = f\n",
    "print(\"Percentage of missing values in STP is\",numerator/denominator)\n",
    "print(\"Maximum Wind Gust in the year 2019 is\")\n",
    "df.select('STATION','DATE','GUST').filter(df['GUST'] == lit(max_WG)).show(1)\n",
    "sys.stdout = stdoutOrigin\n",
    "f.close()\n",
    "print(\"Writing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda42d3a931535945538bbb10c25878fbb2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
